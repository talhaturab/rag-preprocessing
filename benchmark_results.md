| Model                   | Input Size  | Inference Time (s) | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore Precision | BERTScore Recall | BERTScore F1 | LLM Judge Score | LLM Judge Label |
| :---------------------- | :---------- | -----------------: | ------: | ------: | ------: | ------------------: | ---------------: | -----------: | --------------: | :-------------- |
| qwen2.5:0.5b            | 100_tokens  |              1.652 |   0.617 |   0.463 |   0.577 |               0.958 |            0.925 |        0.941 |               5 | Excellent       |
| qwen2.5:0.5b            | 600_tokens  |              6.522 |   0.192 |   0.056 |   0.119 |               0.887 |             0.82 |        0.852 |               4 | Very Good       |
| qwen2.5:0.5b            | 2000_tokens |             30.132 |   0.287 |   0.074 |   0.131 |               0.825 |             0.85 |        0.837 |               4 | Very Good       |
| qwen2.5:3b              | 100_tokens  |             19.331 |   0.642 |   0.357 |   0.528 |               0.959 |            0.936 |        0.947 |               5 | Excellent       |
| qwen2.5:3b              | 600_tokens  |             41.434 |   0.254 |   0.089 |   0.175 |               0.878 |            0.825 |        0.851 |               5 | Excellent       |
| qwen2.5:3b              | 2000_tokens |            133.369 |   0.227 |   0.065 |    0.12 |               0.855 |            0.855 |        0.855 |               5 | Excellent       |
| gemma3:1b               | 100_tokens  |              5.875 |   0.348 |   0.147 |    0.29 |               0.907 |            0.891 |        0.899 |               5 | Excellent       |
| gemma3:1b               | 600_tokens  |             13.514 |   0.192 |   0.062 |   0.141 |               0.868 |            0.817 |        0.842 |               5 | Excellent       |
| gemma3:1b               | 2000_tokens |             36.632 |   0.097 |   0.032 |   0.061 |               0.871 |            0.835 |        0.852 |               4 | Very Good       |
| deepseek-r1:1.5b        | 100_tokens  |             18.735 |   0.393 |   0.195 |    0.28 |               0.851 |            0.882 |        0.866 |               5 | Excellent       |
| deepseek-r1:1.5b        | 600_tokens  |             29.687 |   0.324 |   0.073 |   0.157 |               0.844 |            0.832 |        0.838 |               5 | Excellent       |
| deepseek-r1:1.5b        | 2000_tokens |             77.713 |   0.185 |    0.09 |   0.119 |               0.838 |            0.834 |        0.836 |               2 | Poor            |
| tinyllama:latest        | 100_tokens  |              8.701 |   0.757 |   0.515 |   0.639 |               0.954 |            0.939 |        0.946 |               5 | Excellent       |
| tinyllama:latest        | 600_tokens  |             18.341 |   0.165 |   0.067 |   0.116 |               0.878 |             0.81 |        0.842 |               4 | Very Good       |
| tinyllama:latest        | 2000_tokens |             69.654 |   0.248 |   0.058 |   0.109 |               0.846 |            0.841 |        0.844 |               3 | Fair            |
| llama3.2:latest         | 100_tokens  |             20.038 |    0.55 |   0.313 |    0.47 |               0.916 |             0.91 |        0.913 |               5 | Excellent       |
| llama3.2:latest         | 600_tokens  |             54.355 |   0.365 |   0.186 |   0.238 |               0.854 |             0.83 |        0.842 |               4 | Very Good       |
| llama3.2:latest         | 2000_tokens |            154.641 |   0.308 |   0.208 |    0.24 |               0.854 |            0.855 |        0.855 |               4 | Very Good       |
| smollm2:135m            | 100_tokens  |              1.924 |   0.542 |   0.282 |   0.472 |               0.953 |            0.916 |        0.934 |               4 | Very Good       |
| smollm2:135m            | 600_tokens  |              2.636 |   0.187 |   0.049 |   0.097 |               0.868 |             0.81 |        0.838 |               4 | Very Good       |
| smollm2:135m            | 2000_tokens |              7.616 |   0.084 |   0.023 |   0.052 |               0.855 |            0.823 |        0.838 |               2 | Poor            |
| ollama run smollm2:360m | 100_tokens  |              4.828 |   0.533 |   0.297 |   0.453 |               0.939 |            0.913 |        0.926 |               5 | Excellent       |
| ollama run smollm2:360m | 600_tokens  |               6.45 |   0.176 |   0.066 |   0.118 |               0.883 |            0.819 |         0.85 |               4 | Very Good       |
| ollama run smollm2:360m | 2000_tokens |             17.661 |   0.073 |   0.021 |   0.039 |                0.87 |             0.82 |        0.845 |               4 | Very Good       |
| phi                     | 100_tokens  |             17.585 |   0.532 |   0.289 |   0.338 |               0.946 |            0.923 |        0.934 |               5 | Excellent       |
| phi                     | 600_tokens  |            112.749 |   0.317 |   0.069 |   0.128 |               0.801 |            0.821 |        0.811 |               3 | Fair            |
| phi                     | 2000_tokens |            121.102 |    0.07 |   0.036 |   0.057 |               0.857 |            0.813 |        0.835 |               2 | Poor            |
